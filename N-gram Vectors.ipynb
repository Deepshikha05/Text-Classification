{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram vectors / Vector Model Basics\n",
    "- n-grams are used to describe objects as vectors. This makes it possible to apply geometric, statistical and other mathematical techniques, which are well defined for vectors, but not for objects in general. For example, one of the most common uses is to define a similarity measure between textual documents based on the application of a mathematical function to the vector representations of the documents. \n",
    "- Each document or query is conceptually thought of as a D-dimensional vector of weights, where D is the number of unique N-grams in the corpora and the i-th weight in a vector corresponds to the weight associated with the i-th N-gram.\n",
    "- Weight associated with each unique N-gram is -> wt_k = (ln(f_k) + 1) / NORM \n",
    "\n",
    "where f_k is the number of times the k-th N-gram occurs in the document and NORM is a normalization factor (so that each vector has length one). \n",
    "NORM is basically - sqrt(Sum_over_i(w_i^2))\n",
    "\n",
    "-  Consider the text The mouse ran up the clock. Here, the word unigrams (n = 1) are ['the', 'mouse', 'ran', 'up', 'clock'], the word bigrams (n = 2) are ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock'], and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "- Tokenizing into word unigrams + bigrams provides good accuracy while taking less compute time.\n",
    "\n",
    "##### Vectorization \n",
    "- Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process. \n",
    "\n",
    "Example :  Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
    "Index assigned for every token: {'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,\n",
    "  'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the\n",
    "clock': 8, 'down': 1, 'ran down': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot Encoding\n",
    "- In this approach, each element in the vector corresponds to a unique word (token) in the corpus vocabulary. Then, if the token at a particular index exists in the document, that element is marked as 1. Otherwise, it’s 0. This is essentially a Boolean bag-of-words.\n",
    "- It seems like an odd choice of encoding, but it is used somewhat often. Why? More common frequency-encodings follow a [Zipfian Distribution]() which fails for models that expect normally-distributed features. In such cases, one-hot encoding is superior. It is also a good input to functions that require a particular input range, (e.g. neural network activation functions).\n",
    "- Every sample text is represented as a vector indicating the presence or absence of a token in the text.\n",
    "- Example : 'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Encoding\n",
    "- Every sample text is represented as a vector indicating the count of a token in the text. \n",
    "- Note that the element corresponding to the unigram 'the' (bolded below) now is represented as 2 because the word “the” appears twice in the text.\n",
    "- Example : 'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse Document Frequency\n",
    "- Inverse document frequency is a measure of how important a term is in discriminating documents. \n",
    "- Terms which occur in a lot of documents will not be useful for separating the documents and have a low IDF.\n",
    "- Conversely terms which occur in only a couple documents will have a high IDF. \n",
    "\n",
    "IDF = IDF_k = ln(N/n_k) \n",
    "\n",
    "where N is the number of documents in the corpora and n_k is the number of documents the k-th N-gram occurs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity Between Two Documents\n",
    "Similarity between two documents or queries is measured using the cosine of the angle between the two vectors which represent the documents/queries. Because the vectors have been normalized to unit length, this similarity measure is equivalent to computing the dot product of the two vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tf- Idf Encoding\n",
    "- The problem with the above two approaches is that common words that occur in similar frequencies in all documents (i.e., words that are not particularly unique to the text samples in the dataset) are not penalized. \n",
    "- For example, words like “a” will occur very frequently in all texts. So a higher token count for “the” than for other more meaningful words is not very useful.\n",
    "- Example :  'The mouse ran up the clock' = [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,\n",
    "0.23, 0.33, 0.33]\n",
    "\n",
    "Tf-Idf Encoding is better than the other two in terms of accuracy but it takes more memory (as it uses floating-point representation) and takes more time to compute, especially for large datasets (can take twice as long in some cases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection\n",
    "- When we convert all of the texts in a dataset into word uni+bigram tokens, we may end up with tens of thousands of tokens. Not all of these tokens/features contribute to label prediction. So we can drop certain tokens, for instance those that occur extremely rarely across the dataset. We can also measure feature importance (how much each token contributes to label predictions), and only include the most informative tokens.\n",
    "- There are many statistical functions that take features and the corresponding labels and output the feature importance score. Two commonly used functions are f_classif and chi2. \n",
    "\n",
    "##### Normalization\n",
    "- Normalization converts all feature/sample values to small and similar values. \n",
    "- This simplifies gradient descent convergence in learning algorithms.\n",
    "- Normalization does not add much value in text classification problems. \n",
    "\n",
    "To sum-up\n",
    "- Tokenize text-samples into word uni+bigrams\n",
    "- Vectorize using Tf-Idf Encoding \n",
    "- Select only top 20k features from vectors of tokens by discarding tokens that appear fewer than 2 times and using f_classif to calculate feature importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
