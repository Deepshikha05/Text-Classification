{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A popular and simple method of feature extraction with text data is called the bag-of-words model of text.\n",
    "- It is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "- A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "    - A vocabulary of known words.\n",
    "    - A measure of the presence of known words.\n",
    "- It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "- This approach looks at the histogram of the words within the text, i.e. considering each word count as a feature.\n",
    "- The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "- Collect Data\n",
    "It is as simple as having a collection of documents.\n",
    "\n",
    "\n",
    "- Design the vocabulary\n",
    "The task is to find the unique words out of the collected data.(punctuations are not included and case is ignored.)\n",
    "\n",
    "\n",
    "- Create Document Vectors\n",
    "    - The next step is to score words in each document.\n",
    "    - The objective is to turn each document of free text into a vector to score each word. \n",
    "    - The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present. \n",
    "    - All ordering of the words is nominally discarded and we have a consistent way of extracting features from any document in our corpus, ready for use in modeling.\n",
    "    - New documents that overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "\n",
    "\n",
    "- Managing Vocabulary\n",
    "    - As the vocabulary size increases, so does the vector representation of documents. \n",
    "    - You can imagine that for a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary.\n",
    "    - This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "    - Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms.\n",
    "    - As such, there is pressure to decrease the size of the vocabulary when using a bag-of-words model.\n",
    "    - Simple steps for cleaning the text: \n",
    "        - Ignoring case\n",
    "        - Ignoring punctuation\n",
    "        - Ignoring frequent words that don't contain much information called stop words like \"is\", \"are\" etc.\n",
    "        - Fixing misspelled words\n",
    "        - Reducing words to their stem called stemming - eg. play from playing.\n",
    "    - A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\n",
    "    - In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams.\n",
    "    - Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.\n",
    "     \n",
    " \n",
    "- Scoring Words\n",
    "    - Once a vocabulary has been chosen, the occurrence of words in example documents needs to be scored.\n",
    "    - One very simple approach to scoring: a binary scoring of the presence or absence of words.\n",
    "    - Other scoring methods include:\n",
    "        - Counts - count the number of times each word appears in a document.\n",
    "        - Frequencies - calculate frequency that each word appears in a document out of all the words in the document.\n",
    "\n",
    "\n",
    " - Word Hashing\n",
    "    - We can use a hash representation of known words in our vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, which is in turn the size of the vector representation of the document.\n",
    "    - Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. This is called the “hash trick” or “feature hashing“.\n",
    "    - The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n",
    "\n",
    "\n",
    "- TF-IDF\n",
    "    - A problem with scoring word frequency is that the words that appear more often start to dominate the document even though they do not contain as much information to the model as rarer appearing but perhaps document appearing words.\n",
    "    - Term-Frequency - is a scoring of frequency of the word in the current document.\n",
    "    - Inverse Document Frequency - is a scoring how rare the word is across documents.\n",
    "    - The scores are a weighting where not all words are equally as important or interesting.\n",
    "    - The scores have the effect of highlighting words that are distinct (contain useful information) in a given document.\n",
    "    - Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low.\n",
    "    \n",
    "\n",
    "Limitation of Bag of Words Model\n",
    "\n",
    "- Vocabulary: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
    "- Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
