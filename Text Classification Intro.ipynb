{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ai.stanford.edu/%7Eamaas/data/sentiment/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. \n",
    "These are two examples of topic classification, categorizing a text document into one of a predefined set of topics. In many topic classification problems, this categorization is based primarily on keywords in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of text classification is sentiment analysis, whose goal is to identify the polarity of text content: the type of opinion it expresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to achieve Problem Solving using Machine Learning\n",
    "- Gather Data\n",
    "- Explore your data \n",
    "- Choose a Model\n",
    "- Prepare your model\n",
    "- Build, train and evaluate your model\n",
    "- Tune hyperparameters\n",
    "- Deploy your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Models that can be used for Text Classification\n",
    "- Models can be broadly classified into two categories: those that use word ordering information (sequence models), and ones that just see text as “bags” (sets) of words (n-gram models). \n",
    "- Types of sequence models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their variations. Types of n-gram models include logistic regression, simple multi- layer perceptrons (MLPs, or fully-connected neural networks), gradient boosted trees and support vector machines.\n",
    "- It is  observed that the ratio of “number of samples” (S) to “number of words per sample” (W) correlates with which model performs well.\n",
    "- When the value for this ratio is small (<1500), small multi-layer perceptrons that take n-grams as input perform better or at least as well as sequence models. \n",
    "- MLPs are simple to define and understand, and they take much less compute time than sequence models. \n",
    "- When the value for this ratio is large (>= 1500), use a sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm for Data Preparation and Model Building\n",
    "1. Calculate the number of samples/number of words per sample ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
    "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
    "flowchart below):\n",
    "  - Split the samples into word n-grams; convert the n-grams into vectors.\n",
    "  - Score the importance of the vectors and then select the top 20K using the scores.\n",
    "  - Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
    "   sepCNN model to classify them (right branch in the flowchart below):\n",
    "  - Split the samples into words; select the top 20K words based on their frequency.\n",
    "  - Convert the samples into word sequence vectors.\n",
    "  - If the original number of samples/number of words per sample ratio is less\n",
    "     than 15K, using a fine-tuned pre-trained embedding with the sepCNN\n",
    "     model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find\n",
    "   the best model configuration for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data before it can be fed to the Model\n",
    "- Before the data can be fed to a model, it needs to be transformed to a format that the model can understand. \n",
    "- The information associated with the ordering of samples should not influence the relationship between texts and labels. \n",
    "-  For example, if a dataset is sorted by class and is then split into training/validation sets, these sets will not be representative of the overall distribution of data.\n",
    "- A simple best practice to ensure the model is not affected by data order is to always shuffle the data before doing anything else. \n",
    "- If your data is already split into training and validation sets, make sure to transform your validation data the same way you transform your training data.\n",
    "- If you don’t already have separate training and validation sets, you can split the samples after shuffling; it’s typical to use 80% of the samples for training and 20% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "- Tokenization : Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
    "- Vectorization :     Define a good numerical measure to characterize these texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/deepshikha/Deepshikha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "imdb_data_path = os.path.join(data_path, 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "    for fname in sorted(os.listdir(train_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(train_path, fname)) as f:\n",
    "                train_texts.append(f.read())\n",
    "            train_labels.append(0 if category == 'neg' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "    for fname in sorted(os.listdir(test_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(test_path, fname)) as f:\n",
    "                test_texts.append(f.read())\n",
    "            test_labels.append(0 if category == 'neg' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "random.shuffle(train_texts)\n",
    "random.seed(123)\n",
    "random.shuffle(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
